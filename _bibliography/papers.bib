@inproceedings{hintersdorf2024nemo, 
  pages = { }, 
  year = { 2024 }, 
  booktitle = { ICML 2024 Workshop on Foundation Models in the Wild }, 
  title = { Finding NeMo: Localizing Neurons Responsible For Memorization in Diffusion Models }, 
  author = { Lukas Struppek and Dominik Hintersdorf and Kristian Kersting and Adam Dziedzic and Franziska Boenisch },
  abstract = {Diffusion models (DMs) produce very detailed and high-quality images. Their power results from extensive training on large amounts of data — 
  usually scraped from the internet without proper attribution or consent from content creators. Unfortunately, this practice raises privacy and 
  intellectual property concerns, as DMs can memorize and later reproduce their potentially sensitive or copyrighted training images at inference time. 
  Prior efforts prevent this issue by either changing the input to the diffusion process, thereby preventing the DM from generating memorized samples 
  during inference, or removing the memorized data from training altogether. While those are viable solutions when the DM is developed and deployed 
  in a secure and constantly monitored environment, they hold the risk of adversaries circumventing the safeguards and are not effective when the DM 
  itself is publicly released. To solve the problem, we introduce NeMo, the first method to localize memorization of individual data samples down to 
  the level of neurons in DMs' cross-attention layers. Through our experiments, we make the intriguing finding that in many cases, single neurons 
  are responsible for memorizing particular training samples. By deactivating these memorization neurons, we can avoid the replication of training 
  data at inference time, increase the diversity in the generated outputs, and mitigate the leakage of private and copyrighted data. In this way, our 
  NeMo contributes to a more responsible deployment of DMs.},
  abbr = {ICML FMW},
  code = {https://github.com/ml-research/localizing_memorization_in_diffusion_models},
  pdf = {https://openreview.net/forum?id=5wOrSneuwe},
  equal_contrib_auths = {Struppek, Hintersdorf},
  selected = {false}
}

@inproceedings{hintersdorf2024defending, 
  pages = { }, 
  year = { 2024 }, 
  booktitle = { Proceedings of the 27th European Conference on Artificial Intelligence }, 
  title = { Defending Our Privacy With Backdoors }, 
  author = { Dominik Hintersdorf and Lukas Struppek and Daniel Neider and Kristian Kersting },
  abstract = {The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. 
  One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing 
  specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather 
  easy yet effective defense based on backdoor attacks to remove private information such as names and faces of individuals from vision-language models 
  by fine-tuning them for only a few minutes instead of re-training them from scratch. Specifically, through strategic insertion of backdoors into 
  text encoders, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's actual name. For image 
  encoders, we map embeddings of individuals to be removed from the model to a universal, anonymous embedding. Our empirical results demonstrate the 
  effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. 
  Our approach provides not only a new "dual-use" perspective on backdoor attacks, but also presents a promising avenue to enhance the privacy of 
  individuals within models trained on uncurated web-scraped data.},
  abbr = {ECAI},
  code = {https://github.com/D0miH/Defending-Our-Privacy-With-Backdoors},
  pdf = {https://arxiv.org/pdf/2310.08320v4},
  selected = {true}
}

@article{hintersdorf2024clip_privacy,
  title={Does CLIP Know My Face?}, 
  author={Dominik Hintersdorf and Lukas Struppek and Manuel Brack and Felix Friedrich and Patrick Schramowski and Kristian Kersting},
  year={2024},
  journal = {Journal of Artificial Intelligence Research},
  abstract = 	 {With the rise of deep learning in various applications, privacy concerns around the protection of training data have become a critical 
  area of research. Whereas prior studies have focused on privacy risks in single-modal models, we introduce a novel method to assess privacy for 
  multi-modal models, specifically vision-language models like CLIP. The proposed Identity Inference Attack (IDIA) reveals whether an individual 
  was included in the training data by querying the model with images of the same person. Letting the model choose from a wide variety of possible 
  text labels, the model reveals whether it recognizes the person and, therefore, was used for training.
  Our large-scale experiments on CLIP demonstrate that individuals used for training can be identified with very high accuracy. We confirm that the 
  model has learned to associate names with depicted individuals, implying the existence of sensitive information that can be extracted by adversaries. 
  Our results highlight the need for stronger privacy protection in large-scale models and suggest that IDIAs can be used to prove the unauthorized 
  use of data for training and to enforce privacy laws.},
  abbr = {JAIR},
  code = {https://github.com/d0mih/does-clip-know-my-face},
  pdf = {https://jair.org/index.php/jair/article/view/15461/27060},
  selected = {true}
}

@inproceedings{struppek2024adversarialllm, 
  pages = { }, 
  year = { 2024 }, 
  booktitle = { ICLR 2024 Workshop on Secure and Trustworthy Large Language Models (SeT LLM) }, 
  title = { Exploring the Adversarial Capabilities of Large Language Models }, 
  author = { Lukas Struppek and Minh Hieu Le and Dominik Hintersdorf and Kristian Kersting },
  abstract = {The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation 
  capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of 
  LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether 
  common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp. 
  attacks. More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing 
  safe rails. Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively 
  undermining hate speech detection systems. Our findings carry significant implications for (semi-)autonomous systems relying on LLMs, highlighting 
  potential challenges in their interaction with existing systems and safety measures.},
  abbr = {SeT LLM},
  code = {https://github.com/LukasStruppek/Exploiting-Cultural-Biases-via-Homoglyphs},
  pdf = {https://arxiv.org/pdf/2402.09132},
}

@inproceedings{struppek2024homoglyphs, 
  pages = { }, 
  year = { 2024 }, 
  booktitle = { ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models (DPFM) }, 
  title = { Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis }, 
  author = { Lukas Struppek and Dominik Hintersdorf and Felix Friedrich and Manuel Brack and Patrick Schramowski and Kristian Kersting },
  abstract = {Models for text-to-image synthesis have recently drawn a lot of interest. They are capable of producing high-quality images that depict 
  a variety of concepts and styles when conditioned on textual descriptions. However, these models adopt cultural characteristics associated with 
  specific Unicode scripts from their vast amount of training data, which may not be immediately apparent. We show that by simply inserting single 
  non-Latin characters in the textual description, common models reflect cultural biases in their generated images. We analyze this behavior both 
  qualitatively and quantitatively, and identify a model's text encoder as the root cause of the phenomenon. Such behavior can be interpreted as a 
  model feature, offering users a simple way to customize the image generation and reflect their own cultural background. Yet, malicious users or 
  service providers may also try to intentionally bias the image generation. One goal might be to create racist stereotypes by replacing Latin 
  characters with similarly-looking characters from non-Latin scripts, so-called homoglyphs.},
  abbr = {DPFM},
  code = {https://github.com/LukasStruppek/Exploiting-Cultural-Biases-via-Homoglyphs},
  pdf = {https://openreview.net/pdf?id=VeCTgo5f9q},
}

@inproceedings{struppek23smoothing, 
  pages = { }, 
  year = { 2024 }, 
  booktitle = { The Twelfth International Conference on Learning Representations }, 
  title = { Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks }, 
  author = { Lukas Struppek and Dominik Hintersdorf and Kristian Kersting },
  abstract = {Label smoothing – using softened labels instead of hard ones – is a widely adopted regularization method for deep learning, showing 
  diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. 
  To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative 
  samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive 
  analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even more, we reveal that 
  smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, 
  beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhancing model resilience against MIAs.},
  abbr = {ICLR},
  code = {https://github.com/LukasStruppek/Plug-and-Play-Attacks},
  pdf = {https://openreview.net/pdf?id=1SbkubNdbW}
}

@inproceedings{hintersdorf2024nemo, 
  pages = { }, 
  year = { 2024 }, 
  booktitle = { Proceedings of the 38th Conference on Neural Information Processing Systems (NeurIPS) }, 
  title = { Finding NeMo: Localizing Neurons Responsible For Memorization in Diffusion Models }, 
  author = { Dominik Hintersdorf and Lukas Struppek and Kristian Kersting and Adam Dziedzic and Franziska Boenisch },
  abstract = {Diffusion models (DMs) produce very detailed and high-quality images. Their power results from extensive training on large amounts of data — 
  usually scraped from the internet without proper attribution or consent from content creators. Unfortunately, this practice raises privacy and 
  intellectual property concerns, as DMs can memorize and later reproduce their potentially sensitive or copyrighted training images at inference time. 
  Prior efforts prevent this issue by either changing the input to the diffusion process, thereby preventing the DM from generating memorized samples 
  during inference, or removing the memorized data from training altogether. While those are viable solutions when the DM is developed and deployed 
  in a secure and constantly monitored environment, they hold the risk of adversaries circumventing the safeguards and are not effective when the DM 
  itself is publicly released. To solve the problem, we introduce NeMo, the first method to localize memorization of individual data samples down to 
  the level of neurons in DMs' cross-attention layers. Through our experiments, we make the intriguing finding that in many cases, single neurons 
  are responsible for memorizing particular training samples. By deactivating these memorization neurons, we can avoid the replication of training 
  data at inference time, increase the diversity in the generated outputs, and mitigate the leakage of private and copyrighted data. In this way, our 
  NeMo contributes to a more responsible deployment of DMs.},
  abbr = {NeurIPS},
  code = {https://github.com/ml-research/localizing_memorization_in_diffusion_models},
  pdf = {https://arxiv.org/pdf/2406.02366},
  equal_contrib_auths = {Struppek, Hintersdorf},
  selected = {true}
}

@inproceedings{brack2023sega, 
  booktitle = { Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS) }, 
  pages = { }, 
  year = { 2023 }, 
  author = { Manuel Brack and Felix Friedrich and Dominik Hintersdorf and Lukas Struppek and Patrick Schramowski and Kristian Kersting }, title = { SEGA: Instructing Text-to-Image Models using Semantic Guidance },
  abstract = 	 {Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. 
  However, achieving one-shot generation that aligns with the user’s intent is nearly impossible, yet small changes to the input prompt often result 
  in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion 
  process to flexibly steer it along semantic directions. This semantic guidance (SEGA) generalizes to any generative architecture using classifier-free 
  guidance. More importantly, it allows for subtle and extensive edits, composition and style changes, and optimizing the overall artistic conception. We 
  demonstrate SEGA’s effectiveness on both latent and pixel-based diffusion models such as Stable Diffusion, Paella, and DeepFloyd-IF using a variety of tasks, 
  thus providing strong evidence for its versatility and flexibility.},
  abbr = {NeurIPS},
  code = {https://github.com/ml-research/semantic-image-editing},
  pdf = {https://openreview.net/pdf?id=KIPAIy329j},
}

@inproceedings{uhlig2022dlam, 
  pages = { }, 
  booktitle = { Proceedings of the Annual Digital Forensic Research Workshop USA Conference }, 
  year = { 2023 }, 
  author = { Frieder Uhlig and Lukas Struppek and Dominik Hintersdorf and Thomas Göbel and Harald Baier and Kristian Kersting }, 
  title = { Combining AI and AM – Improving Approximate Matching through Transformer Networks },
  abstract = 	 {Approximate matching (AM) is a concept in digital forensics to determine the similarity between digital artifacts. 
  An important use case of AM is the reliable and efficient detection of case-relevant data structures on a blacklist, if only 
  fragments of the original are available. For instance, if only a cluster of indexed malware is still present during the digital 
  forensic investigation, the AM algorithm shall be able to assign the fragment to the blacklisted malware. However, traditional 
  AM functions like TLSH and ssdeep fail to detect files based on their fragments if the presented piece is relatively small compared 
  to the overall file size. A second well-known issue with traditional AM algorithms is the lack of scaling due to the ever-increasing 
  lookup databases. We propose an improved matching algorithm based on transformer models from the field of natural language processing. 
  We call our approach Deep Learning Approximate Matching (DLAM). As a concept from artificial intelligence (AI), DLAM gets knowledge of 
  characteristic blacklisted patterns during its training phase. Then DLAM is able to detect the patterns in a typically much larger file, 
  that is DLAM focuses on the use case of fragment detection. We reveal that DLAM has three key advantages compared to the prominent 
  conventional approaches TLSH and ssdeep. First, it makes the tedious extraction of known to be bad parts obsolete, which is necessary 
  until now before any search for them with AM algorithms. This allows efficient classification of files on a much larger scale, which is 
  important due to exponentially increasing data to be investigated. Second, depending on the use case, DLAM achieves a similar or even 
  significantly higher accuracy in recovering fragments of blacklisted files. Third, we show that DLAM enables the detection of file correlations 
  in the output of TLSH and ssdeep even for small fragment sizes.},
  abbr = {DFRWS},
  code = {https://github.com/warlmare/DLAM},
  pdf = {https://arxiv.org/pdf/2208.11367.pdf},
  equal_contrib_auths = {Struppek, Hintersdorf, Uhlig}
}

@incollection{hintersdorf23defendingbugs, 
  booktitle = { NeurIPS 2023 Workshop on Backdoors in Deep Learning }, 
  pages = { }, year = { 2023 }, 
  title = { Defending Our Privacy With Backdoors }, 
  author = { Dominik Hintersdorf and Lukas Struppek and Daniel Neider and Kristian Kersting },
  abstract = 	 {The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. 
  One of the concerns is that adversaries can extract information about the training data using privacy attacks. 
  Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. 
  We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. 
  Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. 
  Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. 
  Our approach provides not only a new "dual-use" perspective on backdoor attacks, but also presents a promising avenue to enhance the privacy of individuals within models trained on uncurated web-scraped data.},
  abbr = {BUGS},
  code = {https://github.com/D0miH/Defending-Our-Privacy-With-Backdoors},
  pdf = {https://openreview.net/pdf?id=M4ltSJufXU},
}

@incollection{struppek23leveraging, 
  booktitle = { NeurIPS 2023 Workshop on Backdoors in Deep Learning }, 
  pages = { }, 
  year = { 2023 }, 
  title = { Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data }, 
  author = { Lukas Struppek and Martin B. Hentschel and Clifton Poth and Dominik Hintersdorf and Kristian Kersting },
  abstract = 	 {Backdoor attacks pose a serious security threat for training neural networks as they surreptitiously introduce hidden functionalities into a model. 
  Such backdoors remain silent during inference on clean inputs, evading detection due to inconspicuous behavior. However, once a specific trigger pattern appears 
  in the input data, the backdoor activates, causing the model to execute its concealed function. Detecting such poisoned samples within vast datasets is virtually 
  impossible through manual inspection. To address this challenge, we propose a novel approach that enables model training on potentially poisoned datasets by 
  utilizing the power of recent diffusion models. Specifically, we create synthetic variations of all training samples, leveraging the inherent resilience of 
  diffusion models to potential trigger patterns in the data. By combining this generative approach with knowledge distillation, we produce student models that 
  maintain their general performance on the task while exhibiting robust resistance to backdoor triggers.},
  abbr = {BUGS},
  code = {https://github.com/LukasStruppek/Robust_Training_on_Poisoned_Samples},
  pdf = {https://arxiv.org/pdf/2310.06372.pdf},
}

@article{struppek23caia, 
  pages = { }, 
  year = { 2023 }, 
  journal = { arXiv:2303.09289 }, 
  title = { Class Attribute Inference Attacks: Inferring Sensitive Class Information by Diffusion-Based Attribute Manipulations }, 
  author = { Lukas Struppek and Dominik Hintersdorf and Felix Friedrich and Manuel Brack and Patrick Schramowski and Kristian Kersting },
  abstract = {Neural network-based image classifiers are powerful tools for computer vision tasks, but they inadvertently reveal sensitive attribute 
  information about their classes, raising concerns about their privacy. To investigate this privacy leakage, we introduce the first Class Attribute 
  Inference Attack (Caia), which leverages recent advances in text-to-image synthesis to infer sensitive attributes of individual classes in a 
  black-box setting, while remaining competitive with related white-box attacks. Our extensive experiments in the face recognition domain show that 
  Caia can accurately infer undisclosed sensitive attributes, such as an individual's hair color, gender and racial appearance, which are not part of 
  the training labels. Interestingly, we demonstrate that adversarial robust models are even more vulnerable to such privacy leakage than standard 
  models, indicating that a trade-off between robustness and privacy exists.},
  abbr = {arXiv},
  code = {https://github.com/lukasstruppek/class_attribute_inference_attacks},
  pdf = {https://arxiv.org/pdf/2303.09289.pdf}
}

@article{friedrich23fair, 
  author = { Felix Friedrich and Manuel Brack and Dominik Hintersdorf and Lukas Struppek and Patrick Schramowski and Sasha Luccioni and Kristian Kersting }, title = { Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness },
  pages = { }, 
  year = { 2023 }, 
  journal = { arXiv:2302.10893 },
  abstract = {Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. 
  However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. 
  In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, 
  to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any 
  direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing 
  generative image models on fairness, with no data filtering and additional training required.},
  abbr = {arXiv},
  code = {https://github.com/ml-research/fair-diffusion},
  pdf = {https://arxiv.org/pdf/2302.10893.pdf}
}

@inproceedings{struppek23iccv, 
  author = { Lukas Struppek and Dominik Hintersdorf and Kristian Kersting },
  booktitle = { Proceedings of the 19th IEEE/CVF International Conference on Computer Vision}, 
  pages = { }, 
  year = { 2023 }, 
  title = { Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis }, 
  abstract = {While text-to-image synthesis currently enjoys great popularity among researchers and the general public, the security of these models has been neglected so far. 
  Many text-guided image generation models rely on pre-trained text encoders from external sources, and their users trust that the retrieved models will behave as promised. 
  Unfortunately, this might not be the case. We introduce backdoor attacks against text-guided generative models and demonstrate that their text encoders pose a major tampering risk. 
  Our attacks only slightly alter an encoder so that no suspicious model behavior is apparent for image generations with clean prompts. By then inserting a single character trigger into the prompt, e.g., a non-Latin 
  character or emoji, the adversary can trigger the model to either generate images with pre-defined attributes or images following a hidden, potentially malicious description. 
  We empirically demonstrate the high effectiveness of our attacks on Stable Diffusion and highlight that the injection process of a single backdoor takes less than two minutes. 
  Besides phrasing our approach solely as an attack, it can also force an encoder to forget phrases related to certain concepts, such as nudity or violence, and help to make image generation safer.},
  abbr = {ICCV},
  code = {https://github.com/lukasstruppek/rickrolling-the-artist},
  pdf = {https://arxiv.org/pdf/2211.02408.pdf},
}

@article{struppek23exploiting, 
  author = {Lukas Struppek and Dominik Hintersdorf and Felix Friedrich and Manuel Brack and Patrick Schramowski and Kristian Kersting},
  year = { 2023 }, 
  crossref = { }, 
  title = { Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis }, 
  pages = { }, 
  volume = { }, 
  journal = { Journal of Artificial Intelligence Research}, 
  abstract = {Models for text-to-image synthesis, such as DALL-E~2 and Stable Diffusion, have recently drawn a lot of interest from academia and the general public. 
  These models are capable of producing high-quality images that depict a variety of concepts and styles when conditioned on textual descriptions. 
  However, these models adopt cultural characteristics associated with specific Unicode scripts from their vast amount of training data, which may not be 
  immediately apparent. We show that by simply inserting single non-Latin characters in a textual description, common models reflect cultural stereotypes 
  and biases in their generated images. We analyze this behavior both qualitatively and quantitatively, and identify a model's text encoder as the root cause 
  of the phenomenon. Additionally, malicious users or service providers may try to intentionally bias the image generation to create racist stereotypes by 
  replacing Latin characters with similarly-looking characters from non-Latin scripts, so-called homoglyphs. To mitigate such unnoticed script attacks, we 
  propose a novel homoglyph unlearning method to fine-tune a text encoder, making it robust against homoglyph manipulations.},
  abbr = {JAIR},
  code = {https://github.com/LukasStruppek/Exploiting-Cultural-Biases-via-Homoglyphs},
  pdf = {https://arxiv.org/pdf/2209.08891.pdf}
}

@incollection{hintersdorf2022learning, 
  key = { Best Paper Award at ConPro 2022 }, 
  pages = { }, 
  year = { 2022 }, 
  author = { Dominik Hintersdorf and Lukas Struppek and Daniel Neider and Kristian Kersting }, 
  title = { Investigating the Risks of Client-Side Scanning for the Use Case NeuralHash }, 
  booktitle = { 6th Workshop on Technology and Consumer Protection @ IEEE Symposium on Security and Privacy },
  abstract = 	 {Large, text-conditioned generative diffusion models have recently gained a lot of attention for their impressive performance in generating high-fidelity images from text alone. 
  However, achieving high-quality results is almost unfeasible in a one-shot fashion. On the contrary, text-guided image generation involves the user making many slight changes to inputs in order to 
  iteratively carve out the envisioned image. However, slight changes to the input prompt often lead to entirely different images being generated, and thus the control of the artist is limited 
  in its granularity. To provide flexibility, we present the Stable Artist, an image editing approach enabling fine-grained control of the image generation process. 
  The main component is semantic guidance (SEGA) which steers the diffusion process along variable numbers of semantic directions. This allows for subtle edits to images, changes in 
  composition and style, as well as optimization of the overall artistic conception. Furthermore, SEGA enables probing of latent spaces to gain insights into the representation of 
  concepts learned by the model, even complex ones such as 'carbon emission'. We demonstrate the Stable Artist on several tasks, showcasing high-quality image editing and composition.},
  abbr = {ConPro},
  code = {https://github.com/ml-research/Learning-to-Break-Deep-Perceptual-Hashing},
  pdf = {https://www.ieee-security.org/TC/SPW2022/ConPro/papers/hintersdorf-conpro22.pdf},
  equal_contrib_auths = {Struppek, Hintersdorf}
}

@misc{brack2022Stable, 
  pages = { },
  year = { 2022 }, 
  journal = { arXiv:2212.06013 }, 
  author = { Manuel Brack and Patrick Schramowski and Felix Friedrich and Dominik Hintersdorf and Kristian Kersting }, title = { The Stable Artist: Steering Semantics in Diffusion Latent Space },
  abstract = 	 {Large, text-conditioned generative diffusion models have recently gained a lot of attention for their impressive performance in generating high-fidelity images from text alone. 
  However, achieving high-quality results is almost unfeasible in a one-shot fashion. On the contrary, text-guided image generation involves the user making many slight changes to inputs in order to 
  iteratively carve out the envisioned image. However, slight changes to the input prompt often lead to entirely different images being generated, and thus the control of the artist is limited 
  in its granularity. To provide flexibility, we present the Stable Artist, an image editing approach enabling fine-grained control of the image generation process. 
  The main component is semantic guidance (SEGA) which steers the diffusion process along variable numbers of semantic directions. This allows for subtle edits to images, changes in 
  composition and style, as well as optimization of the overall artistic conception. Furthermore, SEGA enables probing of latent spaces to gain insights into the representation of 
  concepts learned by the model, even complex ones such as 'carbon emission'. We demonstrate the Stable Artist on several tasks, showcasing high-quality image editing and composition.},
  abbr = {arXiv},
  code = {https://github.com/ml-research/semantic-image-editing},
  pdf = {https://arxiv.org/pdf/2212.06013.pdf}
}

@InProceedings{struppek22ppa,
  title = 	 {Plug \& Play Attacks: Towards Robust and Flexible Model Inversion Attacks},
  author =       {Struppek, Lukas and Hintersdorf, Dominik and De Almeida Correira, Antonio and Adler, Antonia and Kersting, Kristian},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {20522--20545},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/struppek22a/struppek22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/struppek22a.html},
  abstract = 	 {Model inversion attacks (MIAs) aim to create synthetic images that reflect the class-wise characteristics from a target classifier’s private training data by exploiting the model’s 
  learned knowledge. Previous research has developed generative MIAs that use generative adversarial networks (GANs) as image priors tailored to a specific target model. This makes the attacks time- and resource-consuming, inflexible, 
  and susceptible to distributional shifts between datasets. To overcome these drawbacks, we present Plug &amp; Play Attacks, which relax the dependency between the target model and image prior, and enable the use of a 
  single GAN to attack a wide range of targets, requiring only minor adjustments to the attack. Moreover, we show that powerful MIAs are possible even with publicly available pre-trained GANs and under strong 
  distributional shifts, for which previous approaches fail to produce meaningful results. Our extensive evaluation confirms the improved robustness and flexibility of Plug &amp; Play Attacks and their ability 
  to create high-quality images revealing sensitive class characteristics.},
  abbr = {ICML},
  code = {https://github.com/LukasStruppek/Plug-and-Play-Attacks},
  pdf = {https://arxiv.org/pdf/2201.12179.pdf}
}


@inproceedings{hintersdorf22toTrust,
  title     = {To Trust or Not To Trust Prediction Scores for Membership Inference Attacks},
  author    = {Hintersdorf, Dominik and Struppek, Lukas and Kersting, Kristian},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {3043--3049},
  year      = {2022},
  doi       = {10.24963/ijcai.2022/422},
  url       = {https://doi.org/10.24963/ijcai.2022/422},
  abstract = {Membership inference attacks (MIAs) aim to determine whether a specific sample was used to train a predictive model. Knowing this may indeed lead to a privacy breach. 
  Most MIAs, however, make use of the model's prediction scores - the probability of each output given some input - following the intuition that the trained model tends to behave differently 
  on its training data. We argue that this is a fallacy for many modern deep network architectures. Consequently, MIAs will miserably fail since overconfidence leads to high false-positive 
  rates not only on known domains but also on out-of-distribution data and implicitly acts as a defense against MIAs. Specifically, using generative adversarial networks, we are able to 
  produce a potentially infinite number of samples falsely classified as part of the training data. In other words, the threat of MIAs is overestimated, and less information is leaked 
  than previously assumed. Moreover, there is actually a trade-off between the overconfidence of models and their susceptibility to MIAs: the more classifiers know when they do not know, 
  making low confidence predictions, the more they reveal the training data.},
  abbr = {IJCAI},
  code = {https://github.com/ml-research/to-trust-or-not-to-trust-prediction-scores-for-membership-inference-attacks},
  equal_contrib_auths = {Struppek, Hintersdorf},
  pdf = {https://arxiv.org/pdf/2111.09076.pdf},
}

@inproceedings{struppek22learning,
author = {Struppek, Lukas and Hintersdorf, Dominik and Neider, Daniel and Kersting, Kristian},
title = {Learning to Break Deep Perceptual Hashing: The Use Case NeuralHash},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533073},
doi = {10.1145/3531146.3533073},
abstract = {Apple recently revealed its deep perceptual hashing system NeuralHash to detect child sexual abuse material (CSAM) on user devices before files are uploaded to its iCloud service. 
Public criticism quickly arose regarding the protection of user privacy and the system’s reliability. In this paper, we present the first comprehensive empirical analysis of deep perceptual hashing based on NeuralHash. 
Specifically, we show that current deep perceptual hashing may not be robust. An adversary can manipulate the hash values by applying slight changes in images, either induced by gradient-based approaches or simply by performing standard image transformations, 
forcing or preventing hash collisions. Such attacks permit malicious actors easily to exploit the detection system: from hiding abusive material to framing innocent users, everything is possible. Moreover, using the hash values, 
inferences can still be made about the data stored on user devices. In our view, based on our results, deep perceptual hashing in its current form is generally not ready for robust client-side scanning and should not be used from a privacy perspective.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {58–69},
numpages = {12},
keywords = {perceptual hashing, neural networks, privacy, neuralhash, client-side scanning, deep learning},
location = {Seoul, Republic of Korea},
series = {FAccT '22},
abbr = {FAccT},
code = {https://github.com/ml-research/Learning-to-Break-Deep-Perceptual-Hashing},
equal_contrib_auths = {Struppek, Hintersdorf},
pdf = {https://arxiv.org/pdf/2111.06628.pdf},
}
